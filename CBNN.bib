@incollection{aguilera2021,
  title = {Programmable {{Fading Memory}} in {{Atomic Switch Systems}} for {{Error Checking Applications}}},
  booktitle = {Reservoir {{Computing}}: {{Theory}}, {{Physical Implementations}}, and {{Applications}}},
  author = {Aguilera, Renato and Sillin, Henry O. and Stieg, Adam Z. and Gimzewski, James K.},
  editor = {Nakajima, Kohei and Fischer, Ingo},
  date = {2021},
  pages = {273--303},
  publisher = {Springer},
  location = {Singapore},
  doi = {10.1007/978-981-13-1687-6_12},
  url = {https://doi.org/10.1007/978-981-13-1687-6_12},
  urldate = {2024-12-10},
  abstract = {Disruptive technology in computational devices is required as the universal computing machines approach quantum mechanical limits. Integration of state-of-the-art memristive devices provides optimal scaling of current technologies beyond this limit through the adoption of neuromorphic models. Universal computing machines pioneered by Alan Turing are strictly based on top-down intelligent design. Neuromorphic models instead engage in bottom-up programmability by emulating mammalian brain design and characteristics. Here we show the design, characterization, and implementation of a massively parallel memristor neuromorphic network based on metal chalcogenide atomic switch network (ASN) systems with key characteristics such as short- and long-term potentiation, power-law dynamics, and scale-free topology.},
  isbn = {978-981-13-1687-6},
  langid = {english}
}

@online{ahmad2020,
  title = {{{GAIT-prop}}: {{A}} Biologically Plausible Learning Rule Derived from Backpropagation of Error},
  shorttitle = {{{GAIT-prop}}},
  author = {Ahmad, Nasir and family=Gerven, given=Marcel A. J., prefix=van, useprefix=false and Ambrogioni, Luca},
  date = {2020-11-05},
  eprint = {2006.06438},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/2006.06438},
  urldate = {2024-11-12},
  abstract = {Traditional backpropagation of error, though a highly successful algorithm for learning in artificial neural network models, includes features which are biologically implausible for learning in real neural circuits. An alternative called target propagation proposes to solve this implausibility by using a top-down model of neural activity to convert an error at the output of a neural network into layer-wise and plausible 'targets' for every unit. These targets can then be used to produce weight updates for network training. However, thus far, target propagation has been heuristically proposed without demonstrable equivalence to backpropagation. Here, we derive an exact correspondence between backpropagation and a modified form of target propagation (GAIT-prop) where the target is a small perturbation of the forward pass. Specifically, backpropagation and GAIT-prop give identical updates when synaptic weight matrices are orthogonal. In a series of simple computer vision experiments, we show near-identical performance between backpropagation and GAIT-prop with a soft orthogonality-inducing regularizer.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\8EXS5WIW\\Ahmad et al. - 2020 - GAIT-prop A biologically plausible learning rule derived from backpropagation of error.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\6ZKLIF72\\2006.html}
}

@article{appeltant2011,
  title = {Information Processing Using a Single Dynamical Node as Complex System},
  author = {Appeltant, L. and Soriano, M.C. and Van Der Sande, G. and Danckaert, J. and Massar, S. and Dambre, J. and Schrauwen, B. and Mirasso, C.R. and Fischer, I.},
  date = {2011-09-13},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {2},
  number = {1},
  pages = {468},
  issn = {2041-1723},
  doi = {10.1038/ncomms1476},
  url = {https://www.nature.com/articles/ncomms1476},
  urldate = {2024-12-10},
  langid = {english},
  file = {C:\Users\kmc07\Zotero\storage\N9DT9PHC\Appeltant et al. - 2011 - Information processing using a single dynamical node as complex system.pdf}
}

@article{appeltant2011a,
  title = {Information Processing Using a Single Dynamical Node as Complex System},
  author = {Appeltant, L. and Soriano, M.C. and Van Der Sande, G. and Danckaert, J. and Massar, S. and Dambre, J. and Schrauwen, B. and Mirasso, C.R. and Fischer, I.},
  date = {2011-09-13},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {2},
  number = {1},
  pages = {468},
  issn = {2041-1723},
  doi = {10.1038/ncomms1476},
  url = {https://www.nature.com/articles/ncomms1476},
  urldate = {2024-12-10},
  langid = {english},
  file = {C:\Users\kmc07\Zotero\storage\92X6RFMS\Appeltant et al. - 2011 - Information processing using a single dynamical node as complex system.pdf}
}

@online{ba2016,
  title = {Using {{Fast Weights}} to {{Attend}} to the {{Recent Past}}},
  author = {Ba, Jimmy and Hinton, Geoffrey and Mnih, Volodymyr and Leibo, Joel Z. and Ionescu, Catalin},
  date = {2016-12-05},
  eprint = {1610.06258},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.1610.06258},
  url = {http://arxiv.org/abs/1610.06258},
  urldate = {2024-12-02},
  abstract = {Until recently, research on artificial neural networks was largely restricted to systems with only two types of variable: Neural activities that represent the current or recent input and weights that learn to capture regularities among inputs, outputs and payoffs. There is no good reason for this restriction. Synapses have dynamics at many different time-scales and this suggests that artificial neural networks might benefit from variables that change slower than activities but much faster than the standard weights. These "fast weights" can be used to store temporary memories of the recent past and they provide a neurally plausible way of implementing the type of attention to the past that has recently proved very helpful in sequence-to-sequence models. By using fast weights we can avoid the need to store copies of neural activity patterns.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\7JX4R85U\\Ba et al. - 2016 - Using Fast Weights to Attend to the Recent Past.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\MKJYDGBJ\\1610.html}
}

@article{bendayanrubin2007,
  title = {Long Memory Lifetimes Require Complex Synapses and Limited Sparseness},
  author = {Ben Dayan Rubin, Daniel D. and Fusi, Stefano},
  date = {2007-11-30},
  journaltitle = {Frontiers in Computational Neuroscience},
  shortjournal = {Front. Comput. Neurosci.},
  volume = {1},
  publisher = {Frontiers},
  issn = {1662-5188},
  doi = {10.3389/neuro.10.007.2007},
  url = {https://www.frontiersin.org/journals/computational-neuroscience/articles/10.3389/neuro.10.007.2007/full},
  urldate = {2024-11-28},
  abstract = {{$<$}p{$>$}Theoretical studies have shown that memories last longer if the neural representations are sparse, that is, when each neuron is selective for a small fraction of the events creating the memories. Sparseness reduces both the interference between stored memories and the number of synaptic modifications which are necessary for memory storage. Paradoxically, in cortical areas like the inferotemporal cortex, where presumably memory lifetimes are longer than in the medial temporal lobe, neural representations are less sparse. We resolve this paradox by analyzing the effects of sparseness on complex models of synaptic dynamics in which there are metaplastic states with different degrees of plasticity. For these models, memory retention in a large number of synapses across multiple neurons is significantly more efficient in case of many metaplastic states, that is, for an elevated degree of complexity. In other words, larger brain regions allow to retain memories for significantly longer times only if the synaptic complexity increases with the total number of synapses. However, the initial memory trace, the one experienced immediately after memory storage, becomes weaker both when the number of metaplastic states increases and when the neural representations become sparser. Such a memory trace must be above a given threshold in order to permit every single neuron to retrieve the information stored in its synapses. As a consequence, if the initial memory trace is reduced because of the increased synaptic complexity, then the neural representations must be less sparse. We conclude that long memory lifetimes allowed by a larger number of synapses require more complex synapses, and hence, less sparse representations, which is what is observed in the brain.{$<$}/p{$>$}},
  langid = {english},
  keywords = {Learning,sparseness,synaptic plasticity},
  file = {C:\Users\kmc07\Zotero\storage\BJYWGE8F\Ben Dayan Rubin and Fusi - 2007 - Long memory lifetimes require complex synapses and limited sparseness.pdf}
}

@article{bengio,
  title = {On the {{Optimization}} of a {{Synaptic Learning Rule}}},
  author = {Bengio, Samy and Bengio, Yoshua and Cloutier, Jocelyn and Gecsei, Jan},
  langid = {english},
  file = {C:\Users\kmc07\Zotero\storage\4SMDED63\Bengio et al. - On the Optimization of a Synaptic Learning Rule.pdf}
}

@inproceedings{bengio1991,
  title = {Learning a Synaptic Learning Rule},
  booktitle = {{{IJCNN-91-Seattle International Joint Conference}} on {{Neural Networks}}},
  author = {Bengio, Y. and Bengio, S. and Cloutier, J.},
  date = {1991-07},
  volume = {ii},
  pages = {969 vol.2-},
  doi = {10.1109/IJCNN.1991.155621},
  url = {https://ieeexplore.ieee.org/document/155621},
  urldate = {2024-10-18},
  abstract = {Summary form only given, as follows. The authors discuss an original approach to neural modeling based on the idea of searching, with learning methods, for a synaptic learning rule which is biologically plausible and yields networks that are able to learn to perform difficult tasks. The proposed method of automatically finding the learning rule relies on the idea of considering the synaptic modification rule as a parametric function. This function has local inputs and is the same in many neurons. The parameters that define this function can be estimated with known learning methods. For this optimization, particular attention is given to gradient descent and genetic algorithms. In both cases, estimation of this function consists of a joint global optimization of the synaptic modification function and the networks that are learning to perform some tasks. Both network architecture and the learning function can be designed within constraints derived from biological knowledge.{$<>$}},
  eventtitle = {{{IJCNN-91-Seattle International Joint Conference}} on {{Neural Networks}}},
  keywords = {Biological system modeling,Biology,Computer science,Genetic algorithms,Learning systems,Neurons},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\AXCLBLYX\\Bengio et al. - 1991 - Learning a synaptic learning rule.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\JZ6EYSBC\\Bengio et al. - 1991 - Learning a synaptic learning rule.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\JLDA4IE5\\155621.html}
}

@inproceedings{bengio1994,
  title = {Use of Genetic Programming for the Search of a New Learning Rule for Neural Networks},
  booktitle = {Proceedings of the {{First IEEE Conference}} on {{Evolutionary Computation}}. {{IEEE World Congress}} on {{Computational Intelligence}}},
  author = {Bengio, S. and Bengio, Y. and Cloutier, J.},
  date = {1994-06},
  pages = {324-327 vol.1},
  doi = {10.1109/ICEC.1994.349932},
  url = {https://ieeexplore.ieee.org/abstract/document/349932},
  urldate = {2024-10-17},
  abstract = {In previous work we explained how to use standard optimization methods such as simulated annealing, gradient descent and genetic algorithms to optimize a parametric function which could be used as a learning rule for neural networks. To use these methods, we had to choose a fixed number of parameters and a rigid form for the learning rule. In this article, we propose to use genetic programming to find not only the values of rule parameters but also the optimal number of parameters and the form of the rule. Experiments on classification tasks suggest genetic programming finds better learning rules than other optimization methods. Furthermore, the best rule found with genetic programming outperformed the well-known backpropagation algorithm for a given set of tasks.{$<>$}},
  eventtitle = {Proceedings of the {{First IEEE Conference}} on {{Evolutionary Computation}}. {{IEEE World Congress}} on {{Computational Intelligence}}},
  keywords = {Backpropagation algorithms,Biological system modeling,Design optimization,Genetic algorithms,Genetic programming,Learning systems,Neural networks,Neurons,Optimization methods,Simulated annealing},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\Y83VM8RA\\Bengio et al. - 1994 - Use of genetic programming for the search of a new learning rule for neural networks.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\48L99IW7\\349932.html}
}

@inproceedings{bengio2006,
  title = {Greedy {{Layer-Wise Training}} of {{Deep Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo},
  date = {2006},
  volume = {19},
  publisher = {MIT Press},
  url = {https://proceedings.neurips.cc/paper_files/paper/2006/hash/5da713a690c067105aeb2fae32403405-Abstract.html},
  urldate = {2024-10-10},
  abstract = {Recent analyses (Bengio, Delalleau, \& Le Roux, 2006; Bengio \& Le Cun, 2007) of modern nonparametric machine learning algorithms that are kernel machines, such as Support Vector Machines (SVMs), graph-based manifold and semi-supervised learning algorithms suggest fundamental limitations of some learning algorithms. The problem is clear in kernel-based approaches when the kernel is "local" (e.g., the Gaussian kernel), i.e., K (x, y ) converges to a constant when ||x - y || increases. These analyses point to the difficulty of learning "highly-varying functions", i.e., functions that have a large number of "variations" in the domain of interest, e.g., they would require a large number of pieces to be well represented by a piecewise-linear approximation. Since the number of pieces can be made to grow exponentially with the number of factors of variations in the input, this is connected with the well-known curse of dimensionality for classical non-parametric learning algorithms (for regression, classification and density estimation). If the shapes of all these pieces are unrelated, one needs enough examples for each piece in order to generalize properly. However, if these shapes are related and can be predicted from each other, "non-local" learning algorithms have the potential to generalize to pieces not covered by the training set. Such ability would seem necessary for learning in complex domains such as Artificial Intelligence tasks (e.g., related to vision, language, speech, robotics). Kernel machines (not only those with a local kernel) have a shallow architecture, i.e., only two levels of data-dependent computational elements. This is also true of feedforward neural networks with a single hidden layer (which can become SVMs when the number of hidden units becomes large (Bengio, Le Roux, Vincent, Delalleau, \& Marcotte, 2006)). A serious problem with shallow architectures is that they can be very inefficient in terms of the number of computational units (e.g., bases, hidden units), and thus in terms of required examples (Bengio \& Le Cun, 2007). One way to represent a highly-varying function compactly (with few parameters) is through the composition of many non-linearities, i.e., with a deep architecture. For example, the parity function with d inputs requires O(2d ) examples and parameters to be represented by a Gaussian SVM (Bengio et al., 2006), O(d2 ) parameters for a one-hidden-layer neural network, O(d) parameters and units for a multi-layer network with O(log2 d) layers, and O(1) parameters with a recurrent neural network. More generally,},
  file = {C:\Users\kmc07\Zotero\storage\RAEBBY6W\Bengio et al. - 2006 - Greedy Layer-Wise Training of Deep Networks.pdf}
}

@article{benna2016,
  title = {Computational Principles of Synaptic Memory Consolidation},
  author = {Benna, Marcus K. and Fusi, Stefano},
  date = {2016-12},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {19},
  number = {12},
  eprint = {27694992},
  eprinttype = {pmid},
  pages = {1697--1706},
  issn = {1546-1726},
  doi = {10.1038/nn.4401},
  abstract = {Memories are stored and retained through complex, coupled processes operating on multiple timescales. To understand the computational principles behind these intricate networks of interactions, we construct a broad class of synaptic models that efficiently harness biological complexity to preserve numerous memories by protecting them against the adverse effects of overwriting. The memory capacity scales almost linearly with the number of synapses, which is a substantial improvement over the square root scaling of previous models. This was achieved by combining multiple dynamical processes that initially store memories in fast variables and then progressively transfer them to slower variables. Notably, the interactions between fast and slow variables are bidirectional. The proposed models are robust to parameter perturbations and can explain several properties of biological memory, including delayed expression of synaptic modifications, metaplasticity, and spacing effects.},
  langid = {english},
  keywords = {Animals,Computer Simulation,Consolidation,Long-term memory,Memory,Memory Consolidation,Models Neurological,Nerve Net,Neuronal Plasticity,Neurons,Synapses},
  file = {C:\Users\kmc07\Zotero\storage\DJIUDPI3\Benna and Fusi - 2016 - Computational principles of synaptic memory consolidation.pdf}
}

@online{brannvall2023,
  title = {{{ReLU}} and {{Addition-based Gated RNN}}},
  author = {Brännvall, Rickard and Forsgren, Henrik and Sandin, Fredrik and Liwicki, Marcus},
  date = {2023-08-10},
  eprint = {2308.05629},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/2308.05629},
  urldate = {2024-11-12},
  abstract = {We replace the multiplication and sigmoid function of the conventional recurrent gate with addition and ReLU activation. This mechanism is designed to maintain long-term memory for sequence processing but at a reduced computational cost, thereby opening up for more efficient execution or larger models on restricted hardware. Recurrent Neural Networks (RNNs) with gating mechanisms such as LSTM and GRU have been widely successful in learning from sequential data due to their ability to capture long-term dependencies. Conventionally, the update based on current inputs and the previous state history is each multiplied with dynamic weights and combined to compute the next state. However, multiplication can be computationally expensive, especially for certain hardware architectures or alternative arithmetic systems such as homomorphic encryption. It is demonstrated that the novel gating mechanism can capture long-term dependencies for a standard synthetic sequence learning task while significantly reducing computational costs such that execution time is reduced by half on CPU and by one-third under encryption. Experimental results on handwritten text recognition tasks furthermore show that the proposed architecture can be trained to achieve comparable accuracy to conventional GRU and LSTM baselines. The gating mechanism introduced in this paper may enable privacy-preserving AI applications operating under homomorphic encryption by avoiding the multiplication of encrypted variables. It can also support quantization in (unencrypted) plaintext applications, with the potential for substantial performance gains since the addition-based formulation can avoid the expansion to double precision often required for multiplication.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\VEAWR2GX\\Brännvall et al. - 2023 - ReLU and Addition-based Gated RNN.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\3XP4LZN6\\2308.html}
}

@inproceedings{confavreux2020,
  title = {A Meta-Learning Approach to (Re)Discover Plasticity Rules That Carve a Desired Function into a Neural Network},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Confavreux, Basile and Zenke, Friedemann and Agnes, Everton and Lillicrap, Timothy and Vogels, Tim},
  date = {2020},
  volume = {33},
  pages = {16398--16408},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2020/hash/bdbd5ebfde4934142c8a88e7a3796cd5-Abstract.html},
  urldate = {2024-10-18},
  abstract = {The search for biologically faithful synaptic plasticity rules has resulted in a large body of models. They are usually inspired by -- and fitted to -- experimental data, but they rarely produce neural dynamics that serve complex functions. These failures suggest that current plasticity models are still under-constrained by existing data. Here, we present an alternative approach that uses meta-learning to discover plausible synaptic plasticity rules. Instead of experimental data, the rules are constrained by the functions they implement and the structure they are meant to produce. Briefly, we parameterize synaptic plasticity rules by a Volterra expansion and then use supervised learning methods (gradient descent or evolutionary strategies) to minimize a problem-dependent loss function that quantifies how effectively a candidate plasticity rule transforms an initially random network into one with the desired function. We first validate our approach by re-discovering previously described plasticity rules, starting at the single-neuron level and Oja’s rule'', a simple Hebbian plasticity rule that captures the direction of most variability of inputs to a neuron (i.e., the first principal component). We expand the problem to the network level and ask the framework to find Oja’s rule together with an anti-Hebbian rule such that an initially random two-layer firing-rate network will recover several principal components of the input space after learning. Next, we move to networks of integrate-and-fire neurons with plastic inhibitory afferents. We train for rules that achieve a target firing rate by countering tuned excitation. Our algorithm discovers a specific subset of the manifold of rules that can solve this task. Our work is a proof of principle of an automated and unbiased approach to unveil synaptic plasticity rules that obey biological constraints and can solve complex functions.},
  file = {C:\Users\kmc07\Zotero\storage\7H328YRP\Confavreux et al. - 2020 - A meta-learning approach to (re)discover plasticity rules that carve a desired function into a neura.pdf}
}

@article{coulombe2017,
  title = {Computing with Networks of Nonlinear Mechanical Oscillators},
  author = {Coulombe, Jean C. and York, Mark C. A. and Sylvestre, Julien},
  date = {2017-06-02},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {12},
  number = {6},
  pages = {e0178663},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0178663},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0178663},
  urldate = {2024-12-10},
  abstract = {As it is getting increasingly difficult to achieve gains in the density and power efficiency of microelectronic computing devices because of lithographic techniques reaching fundamental physical limits, new approaches are required to maximize the benefits of distributed sensors, micro-robots or smart materials. Biologically-inspired devices, such as artificial neural networks, can process information with a high level of parallelism to efficiently solve difficult problems, even when implemented using conventional microelectronic technologies. We describe a mechanical device, which operates in a manner similar to artificial neural networks, to solve efficiently two difficult benchmark problems (computing the parity of a bit stream, and classifying spoken words). The device consists in a network of masses coupled by linear springs and attached to a substrate by non-linear springs, thus forming a network of anharmonic oscillators. As the masses can directly couple to forces applied on the device, this approach combines sensing and computing functions in a single power-efficient device with compact dimensions.},
  langid = {english},
  keywords = {Artificial neural networks,Computer hardware,Computer networks,Computers,Network analysis,Neurons,Signaling networks,Speech signal processing},
  file = {C:\Users\kmc07\Zotero\storage\SGH33W4Y\Coulombe et al. - 2017 - Computing with networks of nonlinear mechanical oscillators.pdf}
}

@incollection{dale2021,
  title = {Reservoir {{Computing}} in {{Material Substrates}}},
  booktitle = {Reservoir {{Computing}}: {{Theory}}, {{Physical Implementations}}, and {{Applications}}},
  author = {Dale, Matthew and Miller, Julian F. and Stepney, Susan and Trefzer, Martin A.},
  editor = {Nakajima, Kohei and Fischer, Ingo},
  date = {2021},
  pages = {141--166},
  publisher = {Springer},
  location = {Singapore},
  doi = {10.1007/978-981-13-1687-6_7},
  url = {https://doi.org/10.1007/978-981-13-1687-6_7},
  urldate = {2024-12-10},
  abstract = {We overview Reservoir Computing (RC) with physical systems from an Unconventional Computing (UC) perspective. We discuss challenges present in both fields, including encoding and representation, or how to manipulate and read information; ways to search large and complex configuration spaces of physical systems; and what makes a “good” computing substrate.},
  isbn = {978-981-13-1687-6},
  langid = {english}
}

@book{dayan2001,
  title = {Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems},
  shorttitle = {Theoretical Neuroscience},
  author = {Dayan, Peter and Abbott, L. F.},
  date = {2001},
  series = {Computational Neuroscience},
  publisher = {Massachusetts Institute of Technology Press},
  location = {Cambridge, Mass},
  isbn = {978-0-262-04199-7},
  langid = {english},
  pagetotal = {460},
  file = {C:\Users\kmc07\Zotero\storage\S6TW4RBW\Dayan and Abbott - 2001 - Theoretical neuroscience computational and mathematical modeling of neural systems.pdf}
}

@incollection{dominey2021,
  title = {Cortico-{{Striatal Origins}} of {{Reservoir Computing}}, {{Mixed Selectivity}}, and {{Higher Cognitive Function}}},
  booktitle = {Reservoir {{Computing}}: {{Theory}}, {{Physical Implementations}}, and {{Applications}}},
  author = {Dominey, Peter Ford},
  editor = {Nakajima, Kohei and Fischer, Ingo},
  date = {2021},
  pages = {29--58},
  publisher = {Springer},
  location = {Singapore},
  doi = {10.1007/978-981-13-1687-6_2},
  url = {https://doi.org/10.1007/978-981-13-1687-6_2},
  urldate = {2024-12-10},
  abstract = {The computational richness and complexity of recurrent neural networks is well known, and has yet to be fully exploited and understood. It is interesting that in this context, one of the most prevalent features of the cerebral cortex is its massive recurrent connectivity. Despite this central principle of cortical organization, it is only slowly becoming recognized that the cortex is a reservoir. Of course there are mechanisms in the cortex that allow for plasticity. But the general model of a reservoir as a recurrent network that creates a high dimensional temporal expansion of its inputs which can then be harvested for extracting the required output is fully achieved by the cortex. Future research will find this obvious. This chapter provides a framework for more clearly understanding this conception of cortex and the corticostriatal system.},
  isbn = {978-981-13-1687-6},
  langid = {english}
}

@inproceedings{faraji2015,
  title = {A Biologically Plausible 3-Factor Learning Rule for Expectation Maximization in Reinforcement Learning and Decision Making},
  author = {Faraji, Mohammad Javad and Preuschoff, K. and Gerstner, W.},
  date = {2015},
  url = {https://www.semanticscholar.org/paper/A-biologically-plausible-3-factor-learning-rule-for-Faraji-Preuschoff/a273093eae4e6c0c05ccffcbb3f61199388ad205},
  urldate = {2024-11-25},
  abstract = {One of the most frequent problems in both decision making and reinforcement learning (RL) is expectation maximization involving functionals such as reward or utility. Generally, these problems consist of computing the optimal solution of a density function. Instead of trying to find this exact solution, a common approach is to approximate it through a learning process. In this work we propose a functional gradient rule for the maximization of a general form of density-dependent functionals using a stochastic gradient ascent algorithm. If a neural network is used for parametrization of the desired density function, the proposed learning rule can be viewed as a modulated Hebbian rule. Such a learning rule is biologically plausible, because it consists of both local and global factors corresponding to the coactivity of pre/post-synaptic neurons and the effect of neuromodulation, respectively. We first apply our technique to standard reward maximization in RL. As expected, this yields the standard policy gradient rule in which parameters of the model are updated proportional to the amount of reward. Next, we use variational free energy as a functional and find that the estimated change in parameters is modulated by a measure of surprise signal. Finally, we propose an information theoretical equivalent of existing models in expected utility maximization, as a standard model of decision making, to incorporate both individual preferences and choice variability. We show that our technique can also be applied into such novel framework.},
  file = {C:\Users\kmc07\Zotero\storage\UZHRDBNL\Faraji et al. - 2015 - A biologically plausible 3-factor learning rule for expectation maximization in reinforcement learni.pdf}
}

@article{fusi2005,
  title = {Cascade Models of Synaptically Stored Memories},
  author = {Fusi, Stefano and Drew, Patrick J. and Abbott, L. F.},
  date = {2005-02-17},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {45},
  number = {4},
  eprint = {15721245},
  eprinttype = {pmid},
  pages = {599--611},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2005.02.001},
  abstract = {Storing memories of ongoing, everyday experiences requires a high degree of plasticity, but retaining these memories demands protection against changes induced by further activity and experience. Models in which memories are stored through switch-like transitions in synaptic efficacy are good at storing but bad at retaining memories if these transitions are likely, and they are poor at storage but good at retention if they are unlikely. We construct and study a model in which each synapse has a cascade of states with different levels of plasticity, connected by metaplastic transitions. This cascade model combines high levels of memory storage with long retention times and significantly outperforms alternative models. As a result, we suggest that memory storage requires synapses with multiple states exhibiting dynamics over a wide range of timescales, and we suggest experimental tests of this hypothesis.},
  langid = {english},
  keywords = {Animals,Humans,Memory,Models Neurological,Neuronal Plasticity,Synapses,Time Factors}
}

@article{fusi2007,
  title = {Limits on the Memory Storage Capacity of Bounded Synapses},
  author = {Fusi, Stefano and Abbott, L. F.},
  date = {2007-04},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {10},
  number = {4},
  eprint = {17351638},
  eprinttype = {pmid},
  pages = {485--493},
  issn = {1097-6256},
  doi = {10.1038/nn1859},
  abstract = {Memories maintained in patterns of synaptic connectivity are rapidly overwritten and destroyed by ongoing plasticity related to the storage of new memories. Short memory lifetimes arise from the bounds that must be imposed on synaptic efficacy in any realistic model. We explored whether memory performance can be improved by allowing synapses to traverse a large number of states before reaching their bounds, or by changing the way these bounds are imposed. In the case of hard bounds, memory lifetimes grow proportional to the square of the number of synaptic states, but only if potentiation and depression are precisely balanced. Improved performance can be obtained without fine tuning by imposing soft bounds, but this improvement is only linear with respect to the number of synaptic states. We explored several other possibilities and conclude that improving memory performance requires a more radical modification of the standard model of memory storage.},
  langid = {english},
  keywords = {Animals,Humans,Memory,Models Neurological,Neuronal Plasticity,Synapses},
  file = {C:\Users\kmc07\Zotero\storage\G2SIPD84\Fusi and Abbott - 2007 - Limits on the memory storage capacity of bounded synapses.pdf}
}

@incollection{gallicchio2021,
  title = {Deep {{Reservoir Computing}}},
  booktitle = {Reservoir {{Computing}}: {{Theory}}, {{Physical Implementations}}, and {{Applications}}},
  author = {Gallicchio, Claudio and Micheli, Alessio},
  editor = {Nakajima, Kohei and Fischer, Ingo},
  date = {2021},
  pages = {77--95},
  publisher = {Springer},
  location = {Singapore},
  doi = {10.1007/978-981-13-1687-6_4},
  url = {https://doi.org/10.1007/978-981-13-1687-6_4},
  urldate = {2024-12-10},
  abstract = {This chapter surveys the recent advancements on the extension of Reservoir Computing toward deep architectures, which is gaining increasing research attention in the neural networks community. Within this context, we focus on describing the major features of Deep Echo State Networks based on the hierarchical composition of multiple reservoirs. The intent is to provide a useful reference to guide applications and further developments of this efficient and effective class of approaches to deal with times-series and more complex data within a unified description and analysis.},
  isbn = {978-981-13-1687-6},
  langid = {english}
}

@article{grant2010,
  title = {Computing Behaviour in Complex Synapses: {{Synapse}} Proteome Complexity and the Evolution of Behaviour and Disease},
  shorttitle = {Computing Behaviour in Complex Synapses},
  author = {Grant, Seth G.N.},
  date = {2010-04-01},
  journaltitle = {The Biochemist},
  volume = {32},
  number = {2},
  pages = {6--9},
  issn = {0954-982X, 1740-1194},
  doi = {10.1042/BIO03202006},
  url = {https://portlandpress.com/biochemist/article/32/2/6/942/Computing-behaviour-in-complex-synapses-Synapse},
  urldate = {2024-11-28},
  abstract = {Synapses are the defining feature of the cellular organization of the brain, connecting each neuron with thousands of others. The great morphological diversity of neurons was recognized in the 19th Century, but only in the last 10 years has the remarkable degree of molecular complexity within synapses become apparent. Over 1000 proteins are found in the proteome of the postsynaptic terminal of mammalian synapses. This complexity is organized into networks providing combinatorial signalling for physiological processes, diversity of synapse types and disease susceptibility, as well as providing a new paradigm for the evolution of the brain.},
  langid = {english},
  file = {C:\Users\kmc07\Zotero\storage\QNQE9DZJ\Grant - 2010 - Computing behaviour in complex synapses Synapse proteome complexity and the evolution of behaviour.pdf}
}

@incollection{hadaeghi2021,
  title = {Neuromorphic {{Electronic Systems}} for~{{Reservoir Computing}}},
  booktitle = {Reservoir {{Computing}}: {{Theory}}, {{Physical Implementations}}, and {{Applications}}},
  author = {Hadaeghi, Fatemeh},
  editor = {Nakajima, Kohei and Fischer, Ingo},
  date = {2021},
  pages = {221--237},
  publisher = {Springer},
  location = {Singapore},
  doi = {10.1007/978-981-13-1687-6_10},
  url = {https://doi.org/10.1007/978-981-13-1687-6_10},
  urldate = {2024-12-10},
  abstract = {This chapter provides a comprehensive survey of the researches and motivations for hardware implementation of reservoir computing (RC) on neuromorphic electronic systems. Due to its computational efficiency and the fact that training amounts to a simple linear regression, both spiking and non-spiking implementations of reservoir computing on neuromorphic hardware have been developed. Here, a review of these experimental studies is provided to illustrate the progress in this area and to address the technical challenges which arise from this specific hardware implementation. Moreover, to deal with the challenges of computation on such unconventional substrates, several lines of potential solutions are presented based on advances in other computational approaches in machine learning.},
  isbn = {978-981-13-1687-6},
  langid = {english}
}

@incollection{hauser2021,
  title = {Physical {{Reservoir Computing}} in {{Robotics}}},
  booktitle = {Reservoir {{Computing}}: {{Theory}}, {{Physical Implementations}}, and {{Applications}}},
  author = {Hauser, Helmut},
  editor = {Nakajima, Kohei and Fischer, Ingo},
  date = {2021},
  pages = {169--190},
  publisher = {Springer},
  location = {Singapore},
  doi = {10.1007/978-981-13-1687-6_8},
  url = {https://doi.org/10.1007/978-981-13-1687-6_8},
  urldate = {2024-12-10},
  abstract = {In recent years, there has been an increasing interest in using the concept of physical reservoir computing in robotics. The idea is to employ the robot’s body and its dynamics as a computational resource. On one hand, this has been driven by the introduction of mathematical frameworks showing how complex mechanical structures can be used to build reservoirs. On the other hand, with the recent advances in smart materials, novel additive manufacturing techniques, and the corresponding rise of soft robotics, a new and much richer set of tools for designing and building robots is now available. Despite the increased interest, however, there is still a wide range of unanswered research questions and a rich area of under-explored applications. We will discuss the current state of the art, the implications of using robot bodies as reservoirs, and the great potential and future directions of physical reservoir computing in robotics.},
  isbn = {978-981-13-1687-6},
  langid = {english}
}

@article{hinton1987,
  title = {Using {{Fast Weights}} to {{Deblur Old Memories}}},
  author = {Hinton, Geoffrey E. and Plaut, David C.},
  date = {1987},
  journaltitle = {Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume = {9},
  number = {0},
  url = {https://escholarship.org/uc/item/0570j1dp},
  urldate = {2024-11-16},
  abstract = {Connectionist models usually have a single weight on each connection. Some interesting newproperties emerge if each connection has two weights: A slowly changing, plastic weight which stores long-term knowledge and a fast-changing, elastic weight which stores temporary knowledge and spontaneously decays towards zero. If a network learns a set of associations and then these associationsare "blurred" by subsequent learning, all the original associations can be "deblurred" by rehearsing on just a few of them. The rehearsal allows the fast weights to take on values that temporarily cancel outthe changes in the slow weights caused by the subsequent learning.},
  langid = {english},
  file = {C:\Users\kmc07\Zotero\storage\8CVBYZCV\Hinton and Plaut - 1987 - Using Fast Weights to Deblur Old Memories.pdf}
}

@inproceedings{hochreiter2001,
  title = {Learning to {{Learn Using Gradient Descent}}},
  booktitle = {Artificial {{Neural Networks}} — {{ICANN}} 2001},
  author = {Hochreiter, Sepp and Younger, A. Steven and Conwell, Peter R.},
  editor = {Dorffner, Georg and Bischof, Horst and Hornik, Kurt},
  date = {2001},
  pages = {87--94},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/3-540-44668-0_13},
  abstract = {This paper introduces the application of gradient descent methods to meta-learning. The concept of “meta-learning”, i.e. of a system that improves or discovers a learning algorithm, has been of interest in machine learning for decades because of its appealing applications. Previous meta-learning approaches have been based on evolutionary methods and, therefore, have been restricted to small models with few free parameters. We make meta-learning in large systems feasible by using recurrent neural networks with their attendant learning routines as meta-learning systems. Our system derived complex well performing learning algorithms from scratch. In this paper we also show that our approach performs non-stationary time series prediction.},
  isbn = {978-3-540-44668-2},
  langid = {english},
  keywords = {Boolean Function,Gradient Descent,Hide Layer,Learning Algorithm,Turing Machine},
  file = {C:\Users\kmc07\Zotero\storage\YAE6ZWU7\Hochreiter et al. - 2001 - Learning to Learn Using Gradient Descent.pdf}
}

@incollection{inubushi2021,
  title = {On the {{Characteristics}} and {{Structures}} of~{{Dynamical Systems Suitable}} for~{{Reservoir Computing}}},
  booktitle = {Reservoir {{Computing}}: {{Theory}}, {{Physical Implementations}}, and {{Applications}}},
  author = {Inubushi, Masanobu and Yoshimura, Kazuyuki and Ikeda, Yoshiaki and Nagasawa, Yuto},
  editor = {Nakajima, Kohei and Fischer, Ingo},
  date = {2021},
  pages = {97--116},
  publisher = {Springer},
  location = {Singapore},
  doi = {10.1007/978-981-13-1687-6_5},
  url = {https://doi.org/10.1007/978-981-13-1687-6_5},
  urldate = {2024-12-10},
  abstract = {We present an overview of mathematical aspects of Reservoir Computing (RC). RC is a machine learning method suitable for physical implementation, which harnesses a type of synchronization, called Common-Signal-Induced Synchronization. A precise criterion for this synchronization is given by a quantity called the conditional Lyapunov exponent. We describe a class of dynamical systems (physical systems) that are utilizable for RC in terms of this quantity. Then, two notions characterizing the information processing performance of RC are illustrated: (i) Edge of Chaos and (ii) Memory-Nonlinearity Trade-off. Based on the notion (ii), a structure of dynamical systems suitable for RC has been proposed. This structure is called the mixture reservoir. We review the structure and show its remarkable information processing performance.},
  isbn = {978-981-13-1687-6},
  langid = {english}
}

@article{ji-an2023,
  title = {Face Familiarity Detection with Complex Synapses},
  author = {Ji-An, Li and Stefanini, Fabio and Benna, Marcus K. and Fusi, Stefano},
  date = {2023-01-20},
  journaltitle = {iScience},
  shortjournal = {iScience},
  volume = {26},
  number = {1},
  pages = {105856},
  issn = {2589-0042},
  doi = {10.1016/j.isci.2022.105856},
  url = {https://www.sciencedirect.com/science/article/pii/S2589004222021290},
  urldate = {2024-11-24},
  abstract = {Synaptic plasticity is a complex phenomenon involving multiple biochemical processes that operate on different timescales. Complexity can greatly increase memory capacity when the variables characterizing the synaptic dynamics have limited precision, as shown in simple memory retrieval problems involving random patterns. Here we turn to a real-world problem, face familiarity detection, and we show that synaptic complexity can be harnessed to store in memory a large number of faces that can be recognized at a later time. The number of recognizable faces grows almost linearly with the number of synapses and quadratically with the number of neurons. Complex synapses outperform simple ones characterized by a single variable, even when the total number of dynamical variables is matched. Complex and simple synapses have distinct signatures that are testable in experiments. Our results indicate that a system with complex synapses can be used in real-world tasks such as face familiarity detection.},
  keywords = {Applied computing,Artificial intelligence,Neuroscience},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\6VCHZD3M\\Ji-An et al. - 2023 - Face familiarity detection with complex synapses.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\JYNIVLV7\\S2589004222021290.html}
}

@online{kaplanis2018c,
  title = {Continual {{Reinforcement Learning}} with {{Complex Synapses}}},
  author = {Kaplanis, Christos and Shanahan, Murray and Clopath, Claudia},
  date = {2018-06-19},
  eprint = {1802.07239},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.1802.07239},
  url = {http://arxiv.org/abs/1802.07239},
  urldate = {2024-11-24},
  abstract = {Unlike humans, who are capable of continual learning over their lifetimes, artificial neural networks have long been known to suffer from a phenomenon known as catastrophic forgetting, whereby new learning can lead to abrupt erasure of previously acquired knowledge. Whereas in a neural network the parameters are typically modelled as scalar values, an individual synapse in the brain comprises a complex network of interacting biochemical components that evolve at different timescales. In this paper, we show that by equipping tabular and deep reinforcement learning agents with a synaptic model that incorporates this biological complexity (Benna \& Fusi, 2016), catastrophic forgetting can be mitigated at multiple timescales. In particular, we find that as well as enabling continual learning across sequential training of two simple tasks, it can also be used to overcome within-task forgetting by reducing the need for an experience replay database.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\IUJ5YR56\\Kaplanis et al. - 2018 - Continual Reinforcement Learning with Complex Synapses.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\7PXRAJMH\\1802.html}
}

@article{metz2019,
  title = {{{META-LEARNING UPDATE RULES FOR UNSUPER- VISED REPRESENTATION LEARNING}}},
  author = {Metz, Luke and Maheswaranathan, Niru and Cheung, Brian and Sohl-Dickstein, Jascha},
  date = {2019},
  abstract = {A major goal of unsupervised learning is to discover data representations that are useful for subsequent tasks, without access to supervised labels during training. Typically, this involves minimizing a surrogate objective, such as the negative log likelihood of a generative model, with the hope that representations useful for subsequent tasks will arise as a side effect. In this work, we propose instead to directly target later desired tasks by meta-learning an unsupervised learning rule which leads to representations useful for those tasks. Specifically, we target semi-supervised classification performance, and we meta-learn an algorithm –an unsupervised weight update rule – that produces representations useful for this task. Additionally, we constrain our unsupervised update rule to a be a biologically-motivated, neuron-local function, which enables it to generalize to different neural network architectures, datasets, and data modalities. We show that the meta-learned update rule produces useful features and sometimes outperforms existing unsupervised learning techniques. We further show that the meta-learned unsupervised update rule generalizes to train networks with different widths, depths, and nonlinearities. It also generalizes to train on data with randomly permuted input dimensions and even generalizes from image datasets to a text task.},
  langid = {english},
  file = {C:\Users\kmc07\Zotero\storage\L86ZJU64\Metz et al. - 2019 - META-LEARNING UPDATE RULES FOR UNSUPER- VISED REPRESENTATION LEARNING.pdf}
}

@book{nakajima2021,
  title = {Reservoir {{Computing}}: {{Theory}}, {{Physical Implementations}}, and {{Applications}}},
  shorttitle = {Reservoir {{Computing}}},
  editor = {Nakajima, Kohei and Fischer, Ingo},
  date = {2021},
  series = {Natural {{Computing Series}}},
  publisher = {Springer},
  location = {Singapore},
  doi = {10.1007/978-981-13-1687-6},
  url = {https://link.springer.com/10.1007/978-981-13-1687-6},
  urldate = {2024-12-10},
  isbn = {978-981-13-1686-9 978-981-13-1687-6},
  langid = {english},
  keywords = {dynamical system,Machine Learning,Neural Networks,Reservoir Computing,Signal Processing,Soft Robotics,spintronics}
}

@incollection{pathak2021,
  title = {Reservoir {{Computing}} for {{Forecasting Large Spatiotemporal Dynamical Systems}}},
  booktitle = {Reservoir {{Computing}}: {{Theory}}, {{Physical Implementations}}, and {{Applications}}},
  author = {Pathak, Jaideep and Ott, Edward},
  editor = {Nakajima, Kohei and Fischer, Ingo},
  date = {2021},
  pages = {117--138},
  publisher = {Springer},
  location = {Singapore},
  doi = {10.1007/978-981-13-1687-6_6},
  url = {https://doi.org/10.1007/978-981-13-1687-6_6},
  urldate = {2024-12-10},
  abstract = {Forecasting of spatiotemporal chaotic dynamical systems is an important problem in several scientific fields. Crucial scientific applications such as weather forecasting and climate modeling depend on the ability to effectively model spatiotemporal chaotic geophysical systems such as the atmosphere and oceans. Recent advances in the field of machine learning have the potential to be an important tool for modeling such systems. In this chapter, we review several key ideas and discuss some reservoir-computing-based architectures for purely data-driven as well as hybrid data-assisted forecasting of chaotic systems with an emphasis on scalability to large, high-dimensional systems.},
  isbn = {978-981-13-1687-6},
  langid = {english}
}

@online{sandler2021,
  title = {Meta-{{Learning Bidirectional Update Rules}}},
  author = {Sandler, Mark and Vladymyrov, Max and Zhmoginov, Andrey and Miller, Nolan and Jackson, Andrew and Madams, Tom and family=Arcas, given=Blaise Aguera, prefix=y, useprefix=false},
  date = {2021-06-11},
  eprint = {2104.04657},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/2104.04657},
  urldate = {2024-11-12},
  abstract = {In this paper, we introduce a new type of generalized neural network where neurons and synapses maintain multiple states. We show that classical gradient-based backpropagation in neural networks can be seen as a special case of a two-state network where one state is used for activations and another for gradients, with update rules derived from the chain rule. In our generalized framework, networks have neither explicit notion of nor ever receive gradients. The synapses and neurons are updated using a bidirectional Hebb-style update rule parameterized by a shared low-dimensional "genome". We show that such genomes can be meta-learned from scratch, using either conventional optimization techniques, or evolutionary strategies, such as CMA-ES. Resulting update rules generalize to unseen tasks and train faster than gradient descent based optimizers for several standard computer vision and synthetic tasks.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\5DLI3HPT\\Sandler et al. - 2021 - Meta-Learning Bidirectional Update Rules.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\TQ5WGHBF\\2104.html}
}

@article{shervani-tabar2023,
  title = {Meta-Learning Biologically Plausible Plasticity Rules with Random Feedback Pathways},
  author = {Shervani-Tabar, Navid and Rosenbaum, Robert},
  date = {2023-03-31},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {14},
  number = {1},
  pages = {1805},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-023-37562-1},
  url = {https://www.nature.com/articles/s41467-023-37562-1},
  urldate = {2024-09-21},
  abstract = {Backpropagation is widely used to train artificial neural networks, but its relationship to synaptic plasticity in the brain is unknown. Some biological models of backpropagation rely on feedback projections that are symmetric with feedforward connections, but experiments do not corroborate the existence of such symmetric backward connectivity. Random feedback alignment offers an alternative model in which errors are propagated backward through fixed, random backward connections. This approach successfully trains shallow models, but learns slowly and does not perform well with deeper models or online learning. In this study, we develop a meta-learning approach to discover interpretable, biologically plausible plasticity rules that improve online learning performance with fixed random feedback connections. The resulting plasticity rules show improved online training of deep models in the low data regime. Our results highlight the potential of meta-learning to discover effective, interpretable learning rules satisfying biological constraints.},
  langid = {english},
  keywords = {Computational science,Learning algorithms},
  file = {C:\Users\kmc07\Zotero\storage\E6MFRN9F\Shervani-Tabar and Rosenbaum - 2023 - Meta-learning biologically plausible plasticity rules with random feedback pathways.pdf}
}

@incollection{singer2021,
  title = {The {{Cerebral Cortex}}: {{A Delay-Coupled Recurrent Oscillator Network}}?},
  shorttitle = {The {{Cerebral Cortex}}},
  booktitle = {Reservoir {{Computing}}: {{Theory}}, {{Physical Implementations}}, and {{Applications}}},
  author = {Singer, Wolf},
  editor = {Nakajima, Kohei and Fischer, Ingo},
  date = {2021},
  pages = {3--28},
  publisher = {Springer},
  location = {Singapore},
  doi = {10.1007/978-981-13-1687-6_1},
  url = {https://doi.org/10.1007/978-981-13-1687-6_1},
  urldate = {2024-12-10},
  abstract = {The refinement of machine learning strategies and deep convolutional networks led to the development of artificial systems whose functions resemble those of natural brains, suggesting that the two systems share the same computational principles. In this chapter, evidence is reviewed which indicates that the computational operations of natural systems differ in some important aspects from those implemented in artificial systems. Natural processing architectures are characterized by recurrence and therefore exhibit high-dimensional, non-linear dynamics. Moreover, they use learning mechanisms that support self-organization. It is proposed that these properties allow for computations that are notoriously difficult to realize in artificial systems. Experimental evidence on the organization and function of the cerebral cortex is reviewed that supports this proposal.},
  isbn = {978-981-13-1687-6},
  langid = {english}
}

@incollection{subramoney2021,
  title = {Reservoirs {{Learn}} to {{Learn}}},
  booktitle = {Reservoir {{Computing}}: {{Theory}}, {{Physical Implementations}}, and {{Applications}}},
  author = {Subramoney, Anand and Scherr, Franz and Maass, Wolfgang},
  editor = {Nakajima, Kohei and Fischer, Ingo},
  date = {2021},
  pages = {59--76},
  publisher = {Springer},
  location = {Singapore},
  doi = {10.1007/978-981-13-1687-6_3},
  url = {https://doi.org/10.1007/978-981-13-1687-6_3},
  urldate = {2024-12-10},
  abstract = {The common procedure in reservoir computing is to take a “found” reservoir, such as a recurrent neural network with randomly chosen synaptic weights or a complex physical device, and to adapt the weights of linear readouts from this reservoir for a particular computing task. We address the question of whether the performance of reservoir computing can be significantly enhanced if one instead optimizes some (hyper)parameters of the reservoir, not for a single task but for the range of all possible tasks in which one is potentially interested, before the weights of linear readouts are optimized for a particular computing task. After all, networks of neurons in the brain are also known to be not randomly connected. Rather, their structure and parameters emerge from complex evolutionary and developmental processes, arguably in a way that enhances the speed and accuracy of subsequent learning of any concrete task that is likely to be essential for the survival of the organism. We apply the Learning-to-Learn (L2L) paradigm to mimic this two-tier process, where a set of (hyper)parameters of the reservoir are optimized for a whole family of learning tasks. We found that this substantially enhances the performance of reservoir computing for the families of tasks that we considered. Furthermore, L2L enables a new form of reservoir learning that tends to enable even faster learning, where not even the weights of readouts need to be adjusted for learning a concrete task. We present demos and performance results of these new forms of reservoir computing for reservoirs that consist of networks of spiking neurons and are hence of particular interest from the perspective of neuroscience and implementations in spike-based neuromorphic hardware. We leave it as an open question of what performance advantage the new methods that we propose provide for other types of reservoirs.},
  isbn = {978-981-13-1687-6},
  langid = {english}
}

@article{tanaka2019,
  title = {Recent Advances in Physical Reservoir Computing: {{A}} Review},
  shorttitle = {Recent Advances in Physical Reservoir Computing},
  author = {Tanaka, Gouhei and Yamane, Toshiyuki and Héroux, Jean Benoit and Nakane, Ryosho and Kanazawa, Naoki and Takeda, Seiji and Numata, Hidetoshi and Nakano, Daiju and Hirose, Akira},
  date = {2019-07-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {115},
  pages = {100--123},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2019.03.005},
  url = {https://www.sciencedirect.com/science/article/pii/S0893608019300784},
  urldate = {2024-12-10},
  abstract = {Reservoir computing is a computational framework suited for temporal/sequential data processing. It is derived from several recurrent neural network models, including echo state networks and liquid state machines. A reservoir computing system consists of a reservoir for mapping inputs into a high-dimensional space and a readout for pattern analysis from the high-dimensional states in the reservoir. The reservoir is fixed and only the readout is trained with a simple method such as linear regression and classification. Thus, the major advantage of reservoir computing compared to other recurrent neural networks is fast learning, resulting in low training cost. Another advantage is that the reservoir without adaptive updating is amenable to hardware implementation using a variety of physical systems, substrates, and devices. In fact, such physical reservoir computing has attracted increasing attention in diverse fields of research. The purpose of this review is to provide an overview of recent advances in physical reservoir computing by classifying them according to the type of the reservoir. We discuss the current issues and perspectives related to physical reservoir computing, in order to further expand its practical applications and develop next-generation machine learning systems.},
  keywords = {Machine learning,Neural networks,Neuromorphic device,Nonlinear dynamical systems,Reservoir computing}
}

@article{tanaka2020,
  title = {Spatially {{Arranged Sparse Recurrent Neural Networks}} for {{Energy Efficient Associative Memory}}},
  author = {Tanaka, Gouhei and Nakane, Ryosho and Takeuchi, Tomoya and Yamane, Toshiyuki and Nakano, Daiju and Katayama, Yasunao and Hirose, Akira},
  date = {2020-01},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {31},
  number = {1},
  pages = {24--38},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2019.2899344},
  url = {https://ieeexplore.ieee.org/document/8667662/?arnumber=8667662},
  urldate = {2024-12-10},
  abstract = {The development of hardware neural networks, including neuromorphic hardware, has been accelerated over the past few years. However, it is challenging to operate very large-scale neural networks with low-power hardware devices, partly due to signal transmissions through a massive number of interconnections. Our aim is to deal with the issue of communication cost from an algorithmic viewpoint and study learning algorithms for energy-efficient information processing. Here, we consider two approaches to finding spatially arranged sparse recurrent neural networks with the high cost-performance ratio for associative memory. In the first approach following classical methods, we focus on sparse modular network structures inspired by biological brain networks and examine their storage capacity under an iterative learning rule. We show that incorporating long-range intermodule connections into purely modular networks can enhance the cost-performance ratio. In the second approach, we formulate for the first time an optimization problem where the network sparsity is maximized under the constraints imposed by a pattern embedding condition. We show that there is a tradeoff between the interconnection cost and the computational performance in the optimized networks. We demonstrate that the optimized networks can achieve a better cost-performance ratio compared with those considered in the first approach. We show the effectiveness of the optimization approach mainly using binary patterns and apply it also to gray-scale image restoration. Our results suggest that the presented approaches are useful in seeking more sparse and less costly connectivity of neural networks for the enhancement of energy efficiency in hardware neural networks.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}} and {{Learning Systems}}},
  keywords = {Associative memory,Biological neural networks,energy efficiency,Hardware,Hopfield network,Integrated circuit interconnections,interconnection cost,iterative learning rule,Neurons,Optimization,Recurrent neural networks,sparse neural networks,sparse optimization}
}

@inproceedings{tong2018,
  title = {Reservoir {{Computing}} with {{Untrained Convolutional Neural Networks}} for {{Image Recognition}}},
  booktitle = {2018 24th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  author = {Tong, Zhiqiang and Tanaka, Gouhei},
  date = {2018-08},
  pages = {1289--1294},
  issn = {1051-4651},
  doi = {10.1109/ICPR.2018.8545471},
  url = {https://ieeexplore.ieee.org/document/8545471},
  urldate = {2024-12-10},
  abstract = {Reservoir computing has attracted much attention for its easy training process as well as its ability to deal with temporal data. A reservoir computing system consists of a reservoir part represented as a sparsely connected recurrent neural network and a readout part represented as a simple regression model. In machine learning tasks, the reservoir part is fixed and only the readout part is trained. Although reservoir computing has been mainly applied to time series prediction and recognition, it can be applied to image recognition as well by considering an image data as a sequence of pixel values. However, to achieve a high performance in image recognition with raw image data, a large-scale reservoir including a large number of neurons is required. This is a bottleneck in terms of computer memory and computational cost. To overcome this bottleneck, we propose a new method which combines reservoir computing with untrained convolutional neural networks. We use an untrained convolutional neural network to transform raw image data into a set of smaller feature maps in a preprocessing step of the reservoir computing. We demonstrate that our method achieves a high classification accuracy in an image recognition task with a much smaller number of trainable parameters compared with a previous study.},
  eventtitle = {2018 24th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  keywords = {Computational modeling,Convolution,Convolutional neural networks,Feature extraction,Image recognition,Reservoirs,Training},
  file = {C:\Users\kmc07\Zotero\storage\J837JLDM\8545471.html}
}

@article{yildiz2012,
  title = {Re-Visiting the Echo State Property},
  author = {Yildiz, Izzet B. and Jaeger, Herbert and Kiebel, Stefan J.},
  date = {2012-11-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {35},
  pages = {1--9},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2012.07.005},
  url = {https://www.sciencedirect.com/science/article/pii/S0893608012001852},
  urldate = {2024-12-15},
  abstract = {An echo state network (ESN) consists of a large, randomly connected neural network, the reservoir, which is driven by an input signal and projects to output units. During training, only the connections from the reservoir to these output units are learned. A key requisite for output-only training is the echo state property (ESP), which means that the effect of initial conditions should vanish as time passes. In this paper, we use analytical examples to show that a widely used criterion for the ESP, the spectral radius of the weight matrix being smaller than unity, is not sufficient to satisfy the echo state property. We obtain these examples by investigating local bifurcation properties of the standard ESNs. Moreover, we provide new sufficient conditions for the echo state property of standard sigmoid and leaky integrator ESNs. We furthermore suggest an improved technical definition of the echo state property, and discuss what practicians should (and should not) observe when they optimize their reservoirs for specific tasks.},
  keywords = {Bifurcation,Diagonally Schur stable,Echo state network,Lyapunov,Spectral radius},
  file = {C:\Users\kmc07\Zotero\storage\SXTS6G84\S0893608012001852.html}
}

@article{yildiz2012a,
  title = {Re-Visiting the Echo State Property},
  author = {Yildiz, Izzet B. and Jaeger, Herbert and Kiebel, Stefan J.},
  date = {2012-11-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {35},
  pages = {1--9},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2012.07.005},
  url = {https://www.sciencedirect.com/science/article/pii/S0893608012001852},
  urldate = {2024-12-15},
  abstract = {An echo state network (ESN) consists of a large, randomly connected neural network, the reservoir, which is driven by an input signal and projects to output units. During training, only the connections from the reservoir to these output units are learned. A key requisite for output-only training is the echo state property (ESP), which means that the effect of initial conditions should vanish as time passes. In this paper, we use analytical examples to show that a widely used criterion for the ESP, the spectral radius of the weight matrix being smaller than unity, is not sufficient to satisfy the echo state property. We obtain these examples by investigating local bifurcation properties of the standard ESNs. Moreover, we provide new sufficient conditions for the echo state property of standard sigmoid and leaky integrator ESNs. We furthermore suggest an improved technical definition of the echo state property, and discuss what practicians should (and should not) observe when they optimize their reservoirs for specific tasks.},
  keywords = {Bifurcation,Diagonally Schur stable,Echo state network,Lyapunov,Spectral radius},
  file = {C:\Users\kmc07\Zotero\storage\28SATRDX\S0893608012001852.html}
}

@article{yildiz2012b,
  title = {Re-Visiting the Echo State Property},
  author = {Yildiz, Izzet B. and Jaeger, Herbert and Kiebel, Stefan J.},
  date = {2012-11},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {35},
  pages = {1--9},
  issn = {08936080},
  doi = {10.1016/j.neunet.2012.07.005},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608012001852},
  urldate = {2024-12-15},
  abstract = {An echo state network (ESN) consists of a large, randomly connected neural network, the reservoir, which is driven by an input signal and projects to output units. During training, only the connections from the reservoir to these output units are learned. A key requisite for output-only training is the echo state property (ESP), which means that the effect of initial conditions should vanish as time passes. In this paper, we use analytical examples to show that a widely used criterion for the ESP, the spectral radius of the weight matrix being smaller than unity, is not sufficient to satisfy the echo state property. We obtain these examples by investigating local bifurcation properties of the standard ESNs. Moreover, we provide new sufficient conditions for the echo state property of standard sigmoid and leaky integrator ESNs. We furthermore suggest an improved technical definition of the echo state property, and discuss what practicians should (and should not) observe when they optimize their reservoirs for specific tasks.},
  langid = {english},
  file = {C:\Users\kmc07\Zotero\storage\DVGN65IF\Yildiz et al. - 2012 - Re-visiting the echo state property.pdf}
}

@online{zotero-512,
  title = {Cascade {{Models}} of {{Synaptically Stored Memories}} - {{ScienceDirect}}},
  url = {https://www.sciencedirect.com/science/article/pii/S0896627305001170},
  urldate = {2024-11-28},
  file = {C:\Users\kmc07\Zotero\storage\CV6YNLRM\S0896627305001170.html}
}

@online{zotero-569,
  title = {Reservoir {{Computing}} with {{Untrained Convolutional Neural Networks}} for {{Image Recognition}} | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/document/8545471},
  urldate = {2024-12-10},
  file = {C:\Users\kmc07\Zotero\storage\I3NV782M\8545471.html}
}

@online{zotero-570,
  title = {Scopus - {{Document}} Details - {{Information}} Processing Using a Single Dynamical Node as Complex System},
  doi = {10.1038/ncomms1476},
  url = {https://www.scopus.com/record/display.uri?eid=2-s2.0-80053397808&origin=inward&txGid=76e2085e73e050207a9891823415f4ab},
  urldate = {2024-12-10},
  abstract = {Elsevier’s Scopus, the largest abstract and citation database of peer-reviewed literature. Search and access research from the science, technology, medicine, social sciences and arts and humanities fields.},
  langid = {american}
}
