---
category: literaturenote
tags: Consolidation, Long-term memory paper
citekey: benna2016a
---
# Computational principles of synaptic memory consolidation
> [!Cite]
> Benna, M. K., & Fusi, S. (2016). Computational principles of synaptic memory consolidation. _Nature Neuroscience_, _19_(12), 1697–1706. [https://doi.org/10.1038/nn.4401](https://doi.org/10.1038/nn.4401)

>[!md]
> **First Author**::Benna, Marcus K. > **Author**::Fusi, Stefano 
> 
> **Title**:: Computational principles of synaptic memory consolidation
> **Year**:: 2016
> **Citekey**:: benna2016a 
> **itemType**:: journalArticle
> **Journal**:: *Nature Neuroscience* 
> **Volume**:: 19 
> **Issue**:: 12 

> [!LINK] 
> 
>  [Full Text PDF](file://C:\Users\kmc07\Zotero\storage\DJIUDPI3\Benna%20and%20Fusi%20-%202016%20-%20Computational%20principles%20of%20synaptic%20memory%20consolidation.pdf)  >  Obsidian Link: @TODO

> [!Abstract]
> 
> The biological mechanisms underlying memory are complex and typically involve multiple molecular processes operating on timescales ranging from fractions of a second to years. The authors show using a mathematical model of synaptic plasticity and consolidation that this complexity can help explain the formidable memory capacity of biological systems.
> > 

==# Annotations====  
%% begin annotations %%  
  
  

  
<mark style="background-color: #ff6666">Quote</mark>  
  
====> a memory system that is continually receiving and storing new information, synaptic strengths representing memories must be protected from being overwritten during the storage of new information.====  
  
<mark style="background-color: #ff6666">Quote</mark>  
  
====> Early memory models10 suggested that networks of neurons connected by simple synapses can preserve a number of memories that scales linearly with the size of the network====  
  
<mark style="background-color: #ff6666">Quote</mark>  
  
====> However, subsequent theoretical analyses3–5 revealed that ignoring the limits on synaptic strengths  imposed on any real biological system, which had appeared to be a harmless assumption in the calculations, was actually a serious flaw. When these limits are included—for example, in the extreme case of binary synapses in which the weight takes only two distinct valuesthe memory capacity grows only logarithmically with the number of synapses N for highly plastic synapses, and as N for rigid synapses that can store only a small amount of information per memory.====  
  
<mark style="background-color: #ff6666">Quote</mark>  
  
====> A possible resolution of this dilemma is to make each synapse complex enough to contain both plastic and rigid components. In many models the plastic components are represented by fast biochemical processes, which can change rapidly to store new memorie====  
  
<mark style="background-color: #ff6666">Quote</mark>  
  
====> e that the readout circuits perform almost optimally. Moreover, we will show that the ideal observer approach predicts the correct scaling properties of the memory capacity of simple neural circuits that actually perform memory retrieval.====  
  
<mark style="background-color: #2ea8e5">Quote</mark>  
  
====> More quantitatively, we define the memory signal as the overlap between the state of the synaptic ensemble and the pattern of synaptic modifications originally imposed by the event being remembered.====  
  
<mark style="background-color: #ff6666">Quote</mark>  
  
====> Previously stored memories, which are assumed to be random and uncorrelated, make the memory trace noisy. Memories that are stored after the tracked one continuously degrade the memory signal and also contribute to its fluctuations.====  
  
<mark style="background-color: #ff6666">Quote</mark>  
  
====> it is instructive to start from an abstract memory model in which the present weight is expressed as a sum of synaptic modifications ∆wa(tl), weighted by a factor r that decreases with the age of the modification t – tl. The signal of the corresponding memory would decay as r(t – tl), while the noise would be approximately proportional to the square root of the variance of wa(t)====  
  
<mark style="background-color: #ff6666">Quote</mark>  
  
====> Var w t w t r t t  a  l tl t  al l  ()  ( )= ( ) ( − )  ∑<  :  ∆2 2====  
  
<mark style="background-color: #ff6666">Quote</mark>  
  
====> A slowly decaying r would enable the synaptic weight to maintain a dependence on a large number of modifications, but it would also induce a large variance for wa(t), potentially arbitrarily large if the sum extends over arbitrarily many modifications. By contrast, fast decays would limit the number of synaptic modifications that are remembered. Therefore, the memory capacity and its growth as a function of N depend crucially on r(t)====  
  
<mark style="background-color: #ff6666">Quote</mark>  
  
====> the slowest power-law decay one can afford while keeping w finite is approximately r(t) ≈ t−1/2====  
  
<mark style="background-color: #ff6666">Quote</mark>  
  
====> Each synaptic variable is represented by the level of liquid in a beaker. The interactions between variables are mediated by tubes that connect the beakers.====  
  
<mark style="background-color: #ff6666">Quote</mark>  
  
====> The first beaker represents the synaptic weight. Potentiation of the synapse is implemented by pouring liquid into it, whereas depression is implemented by removing liquid. As the liquid level deviates from equilibrium, the fluid flow through the tubes will tend to balance the levels in all beakers. The balancing dynamics is fast when the beakers are small and the tubes large, but slow when the beakers are large and the tubes small. A single synaptic modification is remembered as long as the liquid levels remain significantly different from equilibrium.====  
  
<mark style="background-color: #ff6666">Quote</mark>  
  
====> An efficient memory system should have both long memory lifetimes (i.e., long relaxation times) and a large initial memory strength, obtained with a relatively small number m of variables (i.e., beakers).====  
  
<mark style="background-color: #ff6666">Quote</mark>  
  
====> In a homogeneous chain (Fig. 2a), perturbations already decay with the desired 1 t power law, but it requires a large m that grows as the square root of the memory lifetime. This problem can be circumvented by merging exponentially growing groups of beakers into larger ones of equivalent total area (Fig. 2b) and in addition reducing the sizes of the connecting tubes by exponentially increasing factors (Fig. 2c), which implies that the variables describing the system operate on different timescales that increase exponentially as one moves along the chain.====  
  
<mark style="background-color: #ff6666">Quote</mark>  
  
====> For identical beakers and tubes (as in Fig. 2a), the differential equation governing the dynamics of the uk variables becomes the well-known diffusion equation====  
  
<mark style="background-color: #ff6666">Quote</mark>  
  
====> A unit perturbation introduced at time t = 0 and initially located at x = 0 spreads to neighboring locations in the shape of a continually broadening Gaussian peak. Its center remains at x = 0, where we read out the synaptic efficacy, and while the spatial extent of the perturbation grows as t , the peak decays with time as the desired 1 t .====  
  
<mark style="background-color: #ff6666">Quote</mark>  
  
====> However, for this slow decay to continue at late times, the system requires an extended range of x, corresponding to a large number m of variables in the original system. Indeed, the above description of the shape of the perturbation holds only until it reaches the maximum value of x, which occurs at a time of O (m2).====  
  
<mark style="background-color: #ff6666">Quote</mark>  
  
====> One can dramatically reduce the number of required variables by considering an  inhomogeneous diffusion process, with parameters C(x) and g(x) that depend exponentially on x, which leads to perturbations spreading only logarithmically with time====  
  
<mark style="background-color: #ff6666">Quote</mark>  
  
====> It is unrealistic to assume that each dynamical variable uk can vary over an unlimited range and be manipulated with arbitrary precision. Therefore, we discretize the uk variables and impose rigid limits on them====  
  
<mark style="background-color: #ff6666">Quote</mark>  
  
====> Models with arbitrarily complex networks of interactions can be constructed by starting from the undiscretized linear chain model depicted in Figure 2 and then iteratively ramifying it by splitting off and merging branches (Fig. 7a). In Supplementary Note 4 we show that with appropriately chosen parameters these complex models have the same dynamics for the first beaker and therefore the same memory performance as the linear chain models. We also note that they are robust to relatively large perturbations, such as the complete loss of one interaction pathway, which can be partially compensated by parallel branches====  
  
<mark style="background-color: #ff6666">Quote</mark>  
  
====> A long series of LTP induction events can increase the liquid levels in several beakers, making it more difficult to stabilize a subsequent synaptic depression. The different degrees of plasticity (despite equal efficacies immediately after the depression event) are determined by the states of the hidden variables, which were set by the previous history of synaptic modifications.====  
  
<mark style="background-color: #ff6666">Quote</mark>  
  
====> The model can also replicate the empirical phenomena known as spacing effects16,17 (Fig. 7c). The stability of repeatedly stored memories is known to depend on the spacing between the times of memorization. This phenomenon has been observed in behavioral studies16, but also in electrophysiology experiments on synaptic plasticity28,29. In these experiments, when the interval between repetitions is too short or too long, the memories are less stable than when the repetitions are properly spaced====  
  
<mark style="background-color: #ff6666">Quote</mark>  
  
====> These models show that complexity, which is widely observed in biological synapses, is important for achieving long memory lifetimes and strong initial memory traces. Complexity was shown to be beneficial in previous models, both for synaptic9 and for systems level memory consolidation11. In both cases the memory traces were initially stored in fast variables and then progressively transferred to slower variables.====  
  
<mark style="background-color: #ff6666">Quote</mark>  
  
====> The proposed model synapse is complex, as it requires processes that operate on multiple timescales, but their number is relatively small and grows only logarithmically with the memory lifetime. Of note, for a given number of synapses there is an optimal number of synaptic variables (Supplementary Note 8), beyond which the memory performance slowly degrades. This implies that smaller nervous systems may do better with simpler synapses and larger nervous systems can benefit from more complex ones.====  
  
<mark style="background-color: #ff6666">Quote</mark>  
  
====> We also presented the recurrent networks with degraded memory cues and tested that they could correctly retrieve the original memories. The scaling of the memory capacity with Nn was similar to that in the case of unperturbed cues, with the total number of retrievable memories decreasing smoothly with the level of degradation (Supplementary Note 13).====  
  
<mark style="background-color: #ff6666">Quote</mark>  
  
====> Sparseness can increase the memory capacity both for synapses that can vary in a unlimited range43 and for bounded, bistable synapses3. In both cases the number of storable memories scales almost quadratically with the number Nn of neurons when the representations are sparse enough (i.e., when f, the average fraction of active neurons, scales approximately as 1/Nn).====  
  
<mark style="background-color: #ff6666">Quote</mark>  
  
====> However, this capacity increase entails a reduction in the amount of information stored per memory.====  
  
<mark style="background-color: #ff6666">Quote</mark>  
  
====> Our estimates of memory capacity are based on the ideal observer approach, and hence they only provide us with an upper bound on the memory signal. We validated our results in two local circuits, but it remains unclear how to perform this validation in large neural systems respecting the observed sparse connectivity and modular organization of the brain. Scalability has been studied only in specific cases45,46 and is an important future direction for our work.====  
  
  
%% end annotations %%==

## Notes


%% Import Date: 2024-10-10T20:00:53.953+01:00 %%
