# Pre Work
#prework
## Chhapramhc44 Final Report
### Abstract
- BPTT - non-local, past signals
- RTRL (real-time recurrent learning) = BPTT, expensive computationally, non-local, no past signals
- Random-Feedback Local Online (RFLO) - approximation of RTRL using only local but can't learn complex tasks.

One feature of biological synapses that is missing in artificial neural nets is their internal dynamics over multiple timescales.
Investigate whether imbuing an RNNâ€™s connections with complex synapses can allow it to learn effectively in a self-organized way and based on information that is local to each synapse.

2 Approaches to model training - gradient based meta learning (well) and non-gradient optimisation (bad).

Gradient based meta learning
1) Network and synapses carried out in parallel - computationally prohibitive.
2) Supervised approach - learning symbols + gradients generated by BPTT - these for inputs and outputs for synapses to learn. - Very bad p: -0.1 (RCNN), 0.21 (RNN with RFLO), 0.98 (RNN - BPTT).

**Check best performance was actually -0.1**

### Introduction

BPTT = global information - weather prediction talk Richard turner? - Latent variables - O(N^2)
RTRL - forward pass BPTT - O(N^4) - local in time but not local in space (global variables)

Neural network weights are represented by a scalar value which is fixed once the network is fine-tuned to its training data.

Use a complex model of synapses (Benna and Fusi) where internal synaptic dynamics will receive local information and govern the learning of the network. 
Candidate learning signals ((Shervani-Tabar and Rosenbaum) and Murray) will output an update to RNN's weight.
One signal is global error signal (so non-local) argued by Lillicrap et al. - Suggests feedback connections may induce neural activity to approximate these signals.

**Check these papers**

### Background

#### Backpropagation through Time

Just normal equations - ignore.

#### Real-time Recurrent Learning

Summary of Murray paper

![[Pasted image 20240915181258.png]]
RFLO good at small multiples of RNN time constant (**check this - probably a combination of the differential terms**).
Local is issue.
Random feedback - nearly as good as BPTT (also said by Lillicrap et al.).
#### Computational Principles of Synaptic Memory Consolidation, Benna and Fusi

Create synaptic model to preserve models from being overwritten.
Use multiple dynamic processes at different timescales.
Connections are bi-directional.
Synaptic modification, meta-plasticity and spacing effects emerge.

Simplest form - chain of dynamic variables where each variable only interacts with its neighbours in a differential equation.
Can control timescales.

