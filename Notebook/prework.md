# Prework
## Chhapramhc44 final report
### Abstract
- BPTT - non-local, past signals
- RTRL (real-time recurrent learning) = BPTT, expensive computatonally, non-local, no past signals
- Random-Feedback Local Online (RFLO) - approximation of RTRL using only local but can't learn complex tasks.

One feature of biological synapses that is missing in artificial neural nets is their internal dynamics over multiple timescales.
Investigate whether imbuing an RNNâ€™s connections with complex synapses can allow it to learn effectively in a self-organized way and based on information that is local to each synapse.

2 Approaches to model training - gradient based meta learning (well) and non-gradient optimisation (bad).

Gradient based meta learning
1) Network and synapses carried out in parrallel - computationally prohibitive.
2) Supervised approach - learning symbols + gradients generated by BPTT - these for inputs and outputs for synapses to learn. - Very bad p: -0.1 (RCNN), 0.21 (RNN with RFLO), 0.98 (RNN - BPTT).

**Check best performance was actually -0.1**

### Introduction

BPTT = global information - weather prediction talk richard turner? - Latent variables - O(N^2)
RTRL - foward pass BPTT - O(N^4) - local in time but not local in space (global variables)

Neural network weights are represented by a scalar value which is fixed once the network is fine-tuned to its training data.

Use a complex model of synapses (Benna and Fusi) where internal synaptic dynamics will receive local information and govern the learning of the network. 
Candidate learning signals ((Shervani-Tabar and Rosenbaum) and Murray) will output an update to RNN's weight.
One signal is global error signal (so non-local) argued by  Lillicrap et al. - Suggests feedback connections may induce nerual activity to approximate these signals.

**Check these papers**

### Background

#### Backpropagation through Time

Just normal equations - ignore.

#### Real-time Recurrent Learning

This analysis is included in Murray (read this paper).
Basically just the same as BPTT.

#### Local Online Random Feedback

Summary of murry.
Is RTRL with 2 steps.
1) Locality enforced by using non local parts of the loss function.
2) Feedback weight for gradient based learning at replaced with fixed random feedback weights.

